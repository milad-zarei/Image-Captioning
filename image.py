import gradio as gr
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch

# ØªØ´Ø®ÛŒØµ GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Ù…Ø¯Ù„ Ú©Ù¾Ø´Ù†â€ŒØ³Ø§Ø² (BLIP)
caption_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
caption_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

# ØªØ§Ø¨Ø¹ ØªÙˆÙ„ÛŒØ¯ Ú©Ù¾Ø´Ù† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
def generate_caption(image):
    # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ±: ØªØºÛŒÛŒØ± Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„
    image = image.convert("RGB")
    image = image.resize((384, 384))  # BLIP Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø±ÙˆÛŒ Ø§ÛŒÙ† Ø³Ø§ÛŒØ² Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø±Ø¯

    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ±ÙˆØ¯ÛŒ
    inputs = caption_processor(images=image, return_tensors="pt").to(device)

    # ØªÙˆÙ„ÛŒØ¯ Ú©Ù¾Ø´Ù†
    output = caption_model.generate(**inputs, max_length=50)
    caption_en = caption_processor.decode(output[0], skip_special_tokens=True)
    return caption_en

# Ø±Ø§Ø¨Ø· Gradio
iface = gr.Interface(
    fn=generate_caption,
    inputs=gr.Image(type="pil", label="ğŸ“· Please upload The image"),
    outputs=gr.Textbox(label="ğŸ“ Caption (English)"),
    title="ğŸ–¼ï¸ Image Captioning (Optimized)",
    description="Optimized version of BLIP with GPU support and image preprocessing"
)

if __name__ == "__main__":
    iface.launch()
